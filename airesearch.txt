A Technical Analysis of BPSK31 Decoding Anomalies and the Development of a Real-Time Audio Visualization ToolPart I: Deconstructing the PSK31 Decoding Anomaly: An Analysis of "2E0ODJ" vs. "v/"This analysis provides a detailed technical investigation into a reported BPSK31 decoding failure, where the transmitted callsign "2E0ODJ" was erroneously decoded as "v/". The investigation proceeds from the physical layer of the signal up through the data link layer protocol to identify the most probable failure mechanisms. It concludes with a systematic diagnostic regimen to correct the issue.Section 1.1: The BPSK31 Signal Architecture: A Foundation in Phase and PurityTo understand the failure, one must first understand the fundamental architecture of the BPSK31 signal. It is a precisely engineered waveform designed for extremely narrow bandwidth operation, a feature that underpins its excellent weak-signal performance.1 This design rests on three pillars: phase modulation, a specific baud rate, and meticulous amplitude shaping.Core Concepts of the BPSK31 WaveformPhase Shift Keying (PSK): The "PSK" in BPSK31 stands for Phase Shift Keying. Unlike frequency-shift keying (FSK) used in modes like RTTY, which transmits data by switching between two distinct audio tones, PSK encodes information by altering the phase of a single, continuous carrier tone.2 The most common variant, Binary PSK (BPSK), uses only two phase states. A binary '1' is represented by maintaining the current phase of the carrier, while a binary '0' is represented by imparting an instantaneous 180-degree phase reversal.1 This binary phase modulation is the fundamental information-carrying mechanism of the protocol.The 31.25 Baud Rate: The "31" denotes the symbol rate of 31.25 baud, meaning that one bit of information is transmitted every 1/31.25 seconds, or 32 milliseconds.1 This rate was deliberately chosen for two reasons. First, it corresponds to a typing speed of approximately 50 words per minute, making it well-suited for real-time keyboard-to-keyboard conversations.3 Second, the 31.25 Hz rate is an integer sub-multiple of the 8 kHz sampling rate (8000/256=31.25), a standard in early digital signal processing (DSP) systems and computer sound cards, which simplified the generation and decoding of the signal in software.1Amplitude Shaping for Spectral Purity: An unmodified, instantaneous 180-degree phase flip is mathematically equivalent to multiplying the signal by -1, creating sharp transitions in the waveform. These sharp edges generate a broad spectrum of unwanted harmonics, known as "splatter" or "key clicks," which would interfere with adjacent communications. To prevent this, PSK31 employs a crucial technique: cosine amplitude shaping.7 During a phase reversal (a '0' bit), the signal's amplitude is smoothly and gradually reduced to zero at the exact moment of the phase flip and then smoothly raised back to full strength, following the shape of a raised cosine wave.8 This shaping ensures that all transitions are gentle, which confines the signal's energy to an exceptionally narrow bandwidth of approximately 31 Hz to 60 Hz.10 When a continuous stream of '0's is transmitted, the resulting amplitude-shaped waveform appears on a spectrum analyzer as a clean, two-tone signal with tones spaced at +/- 15.625 Hz from the center frequency, with virtually no sideband splatter.7The Critical Link Between Audio Levels and Physical Layer IntegrityThe cosine shaping is the primary defense that gives PSK31 its spectrally clean, non-interfering nature. However, this defense is easily and commonly defeated by a fundamental operator error: overdriving the audio input of the transmitting radio. When the audio signal from the computer's sound card is set to an excessively high level, it forces the transceiver's Automatic Leveling Control (ALC) circuit to engage.11 The ALC is designed to prevent overmodulation in voice modes by compressing or clipping audio peaks. When applied to a precisely shaped digital signal like PSK31, this clipping action is destructive. It shears off the tops and bottoms of the waveform, re-introducing the sharp edges that cosine shaping was designed to eliminate.The result is a distorted, wideband signal that splatters across the band, interfering with other users and, critically, corrupting the integrity of the data being transmitted.12 The bit error rate (BER) at the receiver increases dramatically, not because of poor propagation, but because the transmitted signal was corrupted at its source. Therefore, the audio gain setting is not a simple volume control; it is a primary control for the physical integrity of the transmitted waveform. An incorrect setting directly predisposes the decoder to the very type of error reported, establishing a clear causal link between a common operational mistake and the observed decoding failure.Section 1.2: Varicode: The Language and Logic of PSK31While the physical layer deals with phase shifts and waveforms, the data link layer determines how characters are translated into the ones and zeros that drive those phase shifts. PSK31 uses a unique and elegant encoding scheme called Varicode.Core Concepts of Varicode EncodingVariable-Length Encoding: Varicode is a variable-length character set, conceptually analogous to Morse code. Instead of assigning a fixed number of bits to every character (as ASCII does with 8 bits), Varicode assigns shorter bit patterns to more frequently used characters in the English language and longer patterns to less common ones.3 For example, the lowercase letter 'e' is encoded with the short, two-bit sequence 11, while the less common uppercase 'E' requires the seven-bit sequence 1110111.10 This design optimizes the overall data throughput for conversational text, sending the most common characters in the shortest amount of time.10The "Golden Rule" of Framing: The most critical design feature of Varicode is its method for character separation, or "framing." Unlike older protocols like RTTY that use dedicated start and stop bits for each character, Varicode is self-synchronizing. The boundary between any two characters is unambiguously marked by a sequence of two or more consecutive zeros (00).14 To make this system work, the protocol enforces a "golden rule": the 00 pattern is guaranteed never to appear within the bit sequence of any valid character.3 When a decoder receives a 00 sequence, it knows with absolute certainty that the preceding block of bits constitutes one complete character and that the subsequent bit marks the beginning of the next. This allows for extremely efficient framing with minimal overhead.The Elegant Fragility of VaricodeThe 00 separator is a clever design, but it represents a single point of failure for the entire decoding process. The integrity of a multi-character transmission hinges entirely on the correct detection of every 00 separator. A single bit error, caused by noise, fading, or the signal distortion discussed previously, can have one of two catastrophic consequences:Corruption of a Separator: A bit error can flip a bit within a 00 separator, changing it to 01 or 10. The decoder, no longer seeing the end-of-character marker, will merge the bitstream of the first character with the bitstream of the second, attempting to decode the resulting long, nonsensical string of bits.Creation of a False Separator: A bit error can create a 00 sequence where one did not exist (e.g., 101 becomes 100). The decoder will interpret this as a premature end to the character, decoding a truncated and incorrect character, and then begin interpreting the remainder of the original character as the start of a new one.In either case, the decoder loses its place in the data stream. This is known as a framing error.17 Once a framing error occurs, the decoder is operating on misaligned data, and the rest of the transmission is likely to be rendered as gibberish. This susceptibility to cascading failure from a single bit error explains why a short burst of interference can corrupt not just one character, but the remainder of a message, which is consistent with the "2E0ODJ" -> "v/" report.Section 1.3: A Bit-Level Autopsy of the Transmission FailureTo diagnose the specific failure, we must analyze the bitstreams involved. By comparing the intended transmission with the received output, we can form a concrete hypothesis about the failure mechanism.Varicode Bit-Level TranslationThe following table details the correct BPSK31 Varicode for the transmitted and received character strings, based on the official specification.10 The double zero (00) character separator is shown explicitly.Intended CharacterTransmitted VaricodeReceived CharacterReceived Varicode211101101v1111011(separator)00(separator)00E1110111/110101111(separator)00010110111(separator)00O10101011(separator)00D10110101(separator)00J111111101The full, correct bitstream for the start of the message "2E0..." would be:11101101 00 1110111 00 10110111 00...The bitstream that would produce the received message "v/..." would be:1111011 00 110101111 00...The Framing Error Hypothesis and Coincidental AlignmentA direct, bit-for-bit comparison between the transmitted and received streams reveals a high number of errors and no simple relationship. This makes a simple, persistent error source (like a stuck bit) unlikely. The most plausible explanation is a catastrophic framing error early in the transmission, after which the decoder was operating on a desynchronized bitstream.Consider the following scenario:A burst of radio frequency interference (RFI) or a deep signal fade (QSB) completely obliterates the bits corresponding to the character 2 and its following 00 separator.The decoder's input is effectively silent or random noise during this period. When the signal returns, the decoder begins listening again at some arbitrary point within the bitstream for the next character, E (1110111).Let's hypothesize the decoder misses the first bit of the E and starts listening from the second bit onwards. The incoming stream it now sees is 110111 (from the end of E), followed by the 00 separator, followed by the code for 0 (10110111), and so on.The decoder's input buffer now contains the bit sequence: 110111 00 10110111 00...The decoder scans this stream, looking for a valid Varicode character followed by a 00. It does not find a match for 110111. It continues to process bits. The exact sequence of events is highly dependent on the specific decoder implementation and the nature of the noise.The key takeaway is that once synchronization is lost, the decoder is simply shifting a window across a stream of bits, looking for a pattern in its lookup table that is terminated by a 00. The characters "v" (1111011) and "/" (110101111) that were decoded are almost certainly the result of coincidental alignment. The corrupted, out-of-sync bitstream happened to contain sequences that matched the Varicode for those characters. The received output is not a "translation" of the original message but rather random garbage that happened to look like valid data. This explains why the output is so short and nonsensical; the decoder found two accidental matches and then the rest of the bitstream did not produce any further valid alignments.Ruling Out Polarity InversionA common bug in digital decoders is polarity inversion, where the meanings of '0' and '1' are swapped. We can test if this is a plausible cause. A '0' is a phase shift, and a '1' is no phase shift.1 If the decoder had this reversed, it would invert every bit in the stream. Let's examine the effect on a common character like 'e' (11) and 'a' (1011).e (11) becomes 00.a (1011) becomes 0100.In both cases, the inverted code contains the forbidden 00 sequence. Because Varicode was designed such that no character can contain 00, a simple polarity inversion would render the vast majority of transmitted characters invalid and undecodable upon reception. The fact that the system is decoding any valid (though incorrect) characters like 'v' and '/' strongly indicates that the problem is not a persistent polarity inversion bug, but rather a more transient corruption of the bitstream, as caused by a framing error.Section 1.4: Root Cause Analysis: A Taxonomy of PSK31 Decoding FailuresThe framing error is the direct cause, or "symptom," of the decoding failure. The following is a systematic list of the underlying conditions, or "diseases," that can produce this symptom, ordered from most to least likely.1. Audio Chain Pathologies (Most Likely Cause)The path from the computer's software to the radio's antenna is the most frequent source of PSK31 signal problems.Transmitter Overdrive & ALC Compression: This is the single most common cause of poor PSK31 signals.12 As detailed in Section 1.1, setting the sound card output volume too high causes the radio's ALC to engage, clipping the audio waveform. This generates significant distortion and splatter, dramatically increasing the bit error rate and making the signal difficult or impossible for a remote station to decode correctly. The goal for a clean signal is to adjust the audio drive level such that the radio's ALC meter shows zero or only minimal deflection during transmission.11Operating System Audio "Enhancements": Modern operating systems like Windows often apply audio processing algorithms to microphone and line-in inputs by default. Features such as "Noise Reduction," "Acoustic Echo Cancellation," or "Equalization" are designed to improve the quality of human voice but will severely distort the precise, phase-coherent PSK31 waveform, rendering it undecodable.19 These features must be explicitly disabled in the operating system's sound control panel.Impedance and Level Mismatch: Connecting a sound card's high-level speaker output directly to a radio's sensitive microphone input without proper attenuation can cause severe audio distortion and clipping before the signal even reaches the radio's main circuitry. Furthermore, a lack of galvanic isolation can introduce ground loop hum. Using a proper digital mode interface with audio isolation transformers and resistive attenuators is critical for preventing both audio distortion and hum.52. Symbol Timing Recovery FailureFor the decoder to work, it must perfectly synchronize its internal clock to the incoming 31.25 baud symbol rate. This process, called symbol timing recovery, ensures that it samples the incoming waveform at the precise instant where the bit's energy is maximal.20If the signal is weak, noisy, or suffering from rapid fading (QSB), the decoder's timing recovery loop may struggle to maintain a lock.22 Likewise, if the transmitter's sound card clock is not perfectly accurate, it can introduce a small frequency offset that the receiver must track. A failure in this timing recovery process can cause the decoder to drift, leading it to sample too early or too late. This results in bit slips (missing a bit) or bit insertions (sampling the same bit twice), which immediately destroys character framing and leads to a cascade of errors.23 MATLAB and Simulink models exist that demonstrate sophisticated timing recovery algorithms, such as those using Farrow interpolators, which are necessary for robust decoding.243. Propagation Issues and Radio Frequency Interference (RFI)While PSK31 is designed to be robust against noise and weak signal conditions 1, severe propagation phenomena can still cause errors.Multipath Fading (QSB): When the signal arrives at the receiver via multiple paths of different lengths, the signals can interfere destructively, causing deep and rapid fades that can obliterate entire characters or separators.Phase Disruption: Propagation paths that pass through the aurora (transpolar paths) can experience "auroral flutter," which rapidly and randomly shifts the signal's phase, making a phase-based mode like PSK31 very difficult to decode.1Local RFI: A strong local noise source, such as a plasma television, faulty power supply, or other electrical device, can introduce interference that corrupts the received signal and increases the bit error rate.Section 1.5: A Regimen for Signal Integrity: A Diagnostic and Corrective ChecklistThis section provides an actionable, step-by-step procedure to diagnose and correct the likely causes of the decoding failure. The checks should be performed in order, moving from the computer to the radio.PSK31 Troubleshooting ChecklistThe following table provides a structured diagnostic procedure. Following these steps will resolve the vast majority of PSK31 signal quality issues.SubsystemCheckpointRecommended Setting / ActionRationale & ReferencesPC: Operating SystemAudio EnhancementsNavigate to the Sound Control Panel -> Recording Devices -> Microphone Properties -> Enhancements Tab. Select "Disable all sound effects."OS-level audio processing is designed for voice and will severely distort the precise PSK31 waveform. 19PC: Software (fldigi)Sound Card SelectionIn the software's configuration menu, verify that the correct sound card or USB audio interface is selected for both audio capture (PortAudio In) and playback (PortAudio Out).A common setup error that directs the audio to the wrong hardware. 11PC: Software (fldigi)Waterfall Drive LevelAdjust the input audio level (using the slider next to the waterfall display) so that the background of the waterfall is a dark blue or black. A speckled or bright yellow/red background indicates the input is overdriven.Prevents clipping and distortion within the sound card's analog-to-digital converter. 13InterfaceAudio Levels (PC to Radio)Begin with the PC's main output volume and the software's transmit audio level set very low (e.g., 10-20%).This is the primary control used to adjust the drive level into the radio and prevent ALC action. 12RadioAutomatic Level Control (ALC)Transmit a test tone and slowly increase the PC audio output level until the radio's ALC meter just begins to move, then back it off slightly. The goal is ZERO ALC deflection.This is the single most critical adjustment. Any ALC activity indicates the radio is clipping the signal, causing distortion and splatter. 11RadioRF Power OutputReduce transmitter power to a reasonable level, typically 20-50 watts. PSK31 is a highly efficient mode.High power is unnecessary for most contacts, increases thermal stress on the radio, and exacerbates any existing distortion problems. 5RadioSpeech ProcessorEnsure any speech compression or processing functions are turned OFF.These circuits are designed for SSB voice and will destroy the amplitude and phase characteristics of a digital waveform. 12RadioMode and FilterSet the radio to USB (Upper Sideband). Use a standard SSB filter width (e.g., 2.4 kHz or 2.7 kHz).This is the standard operating convention for HF digital modes like PSK31. 2Common BPSK31 FrequenciesFor testing and operation, the following frequencies are commonly used for BPSK31 activity. All frequencies are USB dial frequencies.Amateur BandUSB Dial Frequency (kHz)80 meters3580.15040 meters7070.150 (Region 1 & 2) / 7040.150 (Region 3)30 meters10142.15020 meters14070.15017 meters18100.15015 meters21070.15012 meters24920.15010 meters28120.150Frequencies derived from multiple sources, including.1 Note that 14070.150 MHz on the 20-meter band is the primary global calling frequency.Part II: A Real-Time Audio Oscilloscope in C and SDL2This section provides a complete software solution for creating a real-time audio oscilloscope application. The design uses the C programming language and the SDL2 library, adhering to the development framework provided.26 The solution includes a detailed architectural overview, an explanation of the audio interface, the complete source code, and compilation instructions.Section 2.1: System Architecture and DesignA robust, real-time audio application requires careful management of data flow between different execution threads. A naive design can easily lead to instability, audio glitches, or crashes. The architecture presented here is based on the producer-consumer pattern, which ensures thread safety and high performance.The Threading Model and the Producer-Consumer PatternThe Simple DirectMedia Layer (SDL) audio subsystem operates in a dedicated, high-priority thread, separate from the application's main thread. This is essential for delivering low-latency, uninterrupted audio processing. The main thread is responsible for handling user input (e.g., closing the window) and rendering graphics to the screen. Attempting to perform slow operations like drawing directly from the high-priority audio thread is a critical design flaw that will cause audio dropouts and potential deadlocks.To solve this, we employ the producer-consumer pattern:The Producer: The SDL audio callback function, which is executed automatically by SDL in the high-priority audio thread. Its sole purpose is to "produce" data by capturing it from the microphone and placing it into a shared data structure.The Consumer: The main application loop, which runs in the main thread. It "consumes" the audio data from the shared structure and uses it to render the waveform on the screen.The Thread-Safe Data Bridge: A Mutex-Protected Circular BufferTo pass data safely from the producer (audio thread) to the consumer (main thread), a simple shared variable is insufficient due to the risk of race conditions, where both threads attempt to access the data simultaneously, leading to corrupted data. The correct data structure for this task is a circular buffer (also known as a ring buffer) protected by a mutex.Circular Buffer: A fixed-size array that logically wraps around on itself. A write pointer indicates where new data is inserted, and a read pointer indicates where data is consumed from. When the write pointer reaches the end of the buffer, it wraps back to the beginning, overwriting the oldest data. This structure is ideal for streaming real-time data.Mutex: An SDL_Mutex is a synchronization primitive that acts as a lock. Before either thread can access the shared circular buffer, it must first acquire the lock. Once it is finished, it releases the lock, allowing the other thread to proceed. This guarantees that only one thread can access the buffer at any given time, preventing race conditions.The design must also consider performance. Holding a mutex lock for a long time can cause the other thread to block. A highly inefficient approach would be for the main thread to lock the mutex, copy the data, render the data, and then unlock. This would block the audio thread for the entire duration of the rendering process, causing audible glitches. The correct, high-performance strategy is for the main (consumer) thread to:Lock the mutex.Quickly perform a memcpy from the shared circular buffer to a local, private buffer.Immediately unlock the mutex.The main thread can then take as much time as it needs to render the waveform using the data in its private buffer, without ever blocking the time-sensitive audio thread. This architectural choice is paramount for creating a smooth and stable application.Section 2.2: Interfacing with the Microphone via SDL2The provided project templates use the SDL2_mixer library, which is excellent for playing pre-recorded audio chunks and music but is not the correct tool for raw audio capture.26 For this application, we will use the core SDL2 audio API directly to interface with the microphone.Initialization: The SDL_Init(SDL_INIT_AUDIO) call remains necessary to initialize the audio subsystem.Opening the Capture Device: The primary function for audio I/O is SDL_OpenAudioDevice(). To open a recording device, it is called with the following key parameters:device: NULL, to request the system's default recording device.iscapture: 1 (or true), to specify that we are opening a capture device, not a playback device.desired: A pointer to an SDL_AudioSpec structure defining the audio format we want.obtained: A pointer to another SDL_AudioSpec structure that SDL will fill with the actual format it was able to provide.allowed_changes: A flag indicating what deviations from the desired spec are acceptable.Specifying the Audio Format: The SDL_AudioSpec structure is critical. We will populate it to request a common audio format, for example:freq: 44100 (sample rate in Hz)format: AUDIO_S16SYS (16-bit signed integer samples in the native endianness of the system)channels: 1 (for mono audio)samples: 1024 (the size of the internal buffer per callback, a power of two is recommended)callback: A function pointer to our custom audio callback function.The Audio Callback Function: We must implement a function with the signature void audio_callback(void* userdata, Uint8* stream, int len). SDL will call this function automatically from its high-priority audio thread whenever a new buffer of audio data is available from the microphone. The stream parameter is a pointer to the raw audio data, and len is its size in bytes. The sole responsibility of this function in our design will be to lock the mutex, copy the data from the stream into our shared circular buffer, and unlock the mutex.Starting Audio Flow: After successfully opening the device, audio capture is started by calling SDL_PauseAudioDevice(device_id, 0).Section 2.3: The Complete main.c ImplementationThe following is the complete, self-contained source code for the audio oscilloscope application. It is designed to be a drop-in replacement for the main.c file in the provided project structure and will compile cleanly with the associated Makefiles.26 The code is extensively commented to explain the purpose of each section, aligning with the architecture described above.C/*
 * main.c - A Real-Time Audio Oscilloscope using C and SDL2
 *
 * This application captures audio from the default microphone and displays
 * it as a waveform (amplitude vs. time). It is designed to be thread-safe
 * and performant, using a producer-consumer model with a mutex-protected
 * circular buffer to pass data from the high-priority audio thread to the
 * main rendering thread.
 *
 * To compile on Linux: gcc main.c -o oscilloscope $(sdl2-config --cflags --libs) -lm
 * To cross-compile for Windows from Linux: See provided Makefile [26]
 */

#define SDL_MAIN_HANDLED
#include <SDL.h>
#include <stdio.h>
#include <string.h>

// --- Configuration Constants ---
#define SCREEN_WIDTH 1024
#define SCREEN_HEIGHT 512
#define AUDIO_BUFFER_SIZE 4096 // Must be a power of two

// --- Global Application State ---
// This structure holds all the data shared between the audio thread (producer)
// and the main thread (consumer). Access to this struct MUST be protected by a mutex.
typedef struct {
    Sint16 buffer; // Circular buffer for 16-bit audio samples
    volatile int write_pos;           // Position to write new data (updated by audio thread)
    SDL_mutex* mutex;                 // Mutex to ensure thread-safe access
} AudioData;

// --- The Audio Callback Function (Producer) ---
// This function is called by SDL in a separate, high-priority thread
// whenever new audio data is available from the capture device.
// Its only job is to copy the incoming data into our circular buffer.
void audio_callback(void* userdata, Uint8* stream, int len) {
    AudioData* audio_data = (AudioData*)userdata;

    // The stream format is AUDIO_S16SYS, so we have 16-bit samples (2 bytes).
    // The number of samples is the total length in bytes divided by 2.
    int num_samples = len / sizeof(Sint16);
    Sint16* incoming_samples = (Sint16*)stream;

    // Lock the mutex before accessing the shared buffer
    if (SDL_LockMutex(audio_data->mutex) == 0) {
        for (int i = 0; i < num_samples; ++i) {
            // Copy each sample into the circular buffer
            audio_data->buffer[audio_data->write_pos] = incoming_samples[i];
            // Advance the write position, wrapping around if necessary
            audio_data->write_pos = (audio_data->write_pos + 1) % AUDIO_BUFFER_SIZE;
        }
        // Unlock the mutex as soon as we are done
        SDL_UnlockMutex(audio_data->mutex);
    }
}

// --- Main Function ---
int main(int argc, char* argv) {
    // --- SDL Initialization ---
    if (SDL_Init(SDL_INIT_VIDEO | SDL_INIT_AUDIO) < 0) {
        fprintf(stderr, "Could not initialize SDL: %s\n", SDL_GetError());
        return 1;
    }

    SDL_Window* window = SDL_CreateWindow("Real-Time Audio Oscilloscope",
        SDL_WINDOWPOS_CENTERED, SDL_WINDOWPOS_CENTERED,
        SCREEN_WIDTH, SCREEN_HEIGHT, SDL_WINDOW_SHOWN);
    if (!window) {
        fprintf(stderr, "Could not create window: %s\n", SDL_GetError());
        SDL_Quit();
        return 1;
    }

    SDL_Renderer* renderer = SDL_CreateRenderer(window, -1,
        SDL_RENDERER_ACCELERATED | SDL_RENDERER_PRESENTVSYNC);
    if (!renderer) {
        fprintf(stderr, "Could not create renderer: %s\n", SDL_GetError());
        SDL_DestroyWindow(window);
        SDL_Quit();
        return 1;
    }

    // --- Audio System Setup ---
    AudioData audio_data;
    audio_data.write_pos = 0;
    memset(audio_data.buffer, 0, sizeof(audio_data.buffer)); // Clear the buffer

    // Create the mutex for thread-safe data sharing
    audio_data.mutex = SDL_CreateMutex();
    if (!audio_data.mutex) {
        fprintf(stderr, "Could not create mutex: %s\n", SDL_GetError());
        //... cleanup...
        return 1;
    }

    // Specify the desired audio format for capture
    SDL_AudioSpec desired_spec, obtained_spec;
    SDL_zero(desired_spec);
    desired_spec.freq = 44100;
    desired_spec.format = AUDIO_S16SYS; // Signed 16-bit, system endian
    desired_spec.channels = 1; // Mono
    desired_spec.samples = 1024; // Buffer size
    desired_spec.callback = audio_callback;
    desired_spec.userdata = &audio_data;

    // Open the default audio capture device
    SDL_AudioDeviceID dev = SDL_OpenAudioDevice(NULL, 1, &desired_spec, &obtained_spec, 0);
    if (dev == 0) {
        fprintf(stderr, "Failed to open audio device: %s\n", SDL_GetError());
        //... cleanup...
        return 1;
    }

    // Start audio capture
    SDL_PauseAudioDevice(dev, 0);

    // --- Main Loop (Consumer) ---
    int is_running = 1;
    Sint16 local_buffer; // A private buffer for rendering

    while (is_running) {
        // --- Event Handling ---
        SDL_Event e;
        while (SDL_PollEvent(&e)!= 0) {
            if (e.type == SDL_QUIT) {
                is_running = 0;
            }
        }

        // --- Data Consumption ---
        // Lock the mutex, quickly copy the data we need for this frame, then unlock.
        // This minimizes the time the audio thread might be blocked.
        if (SDL_LockMutex(audio_data.mutex) == 0) {
            // We want to draw the most recent SCREEN_WIDTH samples.
            // We calculate the starting position in the circular buffer.
            int start_pos = (audio_data.write_pos - SCREEN_WIDTH + AUDIO_BUFFER_SIZE) % AUDIO_BUFFER_SIZE;
            for (int i = 0; i < SCREEN_WIDTH; ++i) {
                local_buffer[i] = audio_data.buffer;
            }
            SDL_UnlockMutex(audio_data.mutex);
        }

        // --- Drawing ---
        // Set background color (dark blue)
        SDL_SetRenderDrawColor(renderer, 20, 20, 40, 255);
        SDL_RenderClear(renderer);

        // Set waveform color (bright green)
        SDL_SetRenderDrawColor(renderer, 100, 255, 100, 255);

        // Draw the waveform from our local buffer
        for (int x = 0; x < SCREEN_WIDTH - 1; ++x) {
            // Scale the 16-bit sample value (-32768 to 32767) to the screen height
            // The center of the screen is SCREEN_HEIGHT / 2
            int y1 = (int)((local_buffer[x] / 32767.0f) * (SCREEN_HEIGHT / 2.0f) + (SCREEN_HEIGHT / 2.0f));
            int y2 = (int)((local_buffer[x + 1] / 32767.0f) * (SCREEN_HEIGHT / 2.0f) + (SCREEN_HEIGHT / 2.0f));
            
            SDL_RenderDrawLine(renderer, x, y1, x + 1, y2);
        }

        // --- Update Screen ---
        SDL_RenderPresent(renderer);
    }

    // --- Cleanup ---
    SDL_PauseAudioDevice(dev, 1); // Stop audio capture
    SDL_CloseAudioDevice(dev);
    SDL_DestroyMutex(audio_data.mutex);
    SDL_DestroyRenderer(renderer);
    SDL_DestroyWindow(window);
    SDL_Quit();

    return 0;
}
Section 2.4: Compilation and Execution GuideThis section provides clear, copy-paste instructions for building and running the oscilloscope application in the two specified development environments.Instructions for Windows (Cross-Compilation from Linux)This procedure uses the cross-compilation framework provided.26Create a new directory for your project (e.g., mkdir audio_scope && cd audio_scope).Save the C code from Section 2.3 as main.c inside this new directory.Copy the Makefile template from your framework documentation 26 into the same directory.Open the Makefile in a text editor and change the TARGET variable:Change TARGET = game.exe to TARGET = oscilloscope.exe.From the terminal, inside your project directory, run the make command:Bashmake
This will produce the Windows executable oscilloscope.exe. This file can be copied to and run on any modern Windows machine.Instructions for Linux (Native Compilation)This procedure uses the native Linux Makefile provided.26Create a new directory for your project (e.g., mkdir audio_scope && cd audio_scope).Save the C code from Section 2.3 as main.c inside this new directory.Copy the Makefile.linux.txt file 26 into the directory, renaming it to Makefile.Open the Makefile in a text editor and change the TARGET variable:Change TARGET = pingoscope to TARGET = oscilloscope.From the terminal, inside your project directory, run the make command:Bashmake
This will produce the native Linux executable oscilloscope. Run it from the terminal:./oscilloscope```Note on Library Linking: Both Makefiles are configured to link against the SDL2_mixer and SDL2_ttf libraries. While this specific oscilloscope application does not use functions from those libraries, leaving the -lSDL2_mixer and -lSDL2_ttf flags in the linker command is harmless and maintains consistency with the established project framework for future expansion.
